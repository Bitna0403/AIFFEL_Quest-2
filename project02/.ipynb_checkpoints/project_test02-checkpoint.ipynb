{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "755ada08",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (357091280.py, line 143)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_99/357091280.py\"\u001b[0;36m, line \u001b[0;32m143\u001b[0m\n\u001b[0;31m    {'model':model_b, 'name':'LightGBM'}, {'model':model_d, 'name':'RandomForest'}]random_state=random_state)\u001b[0m\n\u001b[0m                                                                                   ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import missingno as msno\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\n",
    "\n",
    "train_data_path = join(data_dir, 'train.csv')\n",
    "test_data_path = join(data_dir, 'test.csv') \n",
    "\n",
    "train = pd.read_csv(train_data_path)\n",
    "test = pd.read_csv(test_data_path)\n",
    "\n",
    "# 데이터 전처리 \n",
    "train['date'] = train['date'].apply(lambda i: i[:6]).astype(int)\n",
    "\n",
    "y = train['price']\n",
    "del train['price']\n",
    "del train['id']\n",
    "\n",
    "test['date'] = test['date'].apply(lambda i: i[:6]).astype(int)\n",
    "del test['id']\n",
    "\n",
    "# seaborn의 `kdeplot`을 활용해 y분포 확인\n",
    "sns.kdeplot(y)\n",
    "plt.show()\n",
    "\n",
    "# 왼쪽으로 크게 치우쳐 있는 y값 로그변환 \n",
    "y = np.log1p(y)\n",
    "\n",
    "# 데이터 형태 확인\n",
    "train.info()\n",
    "\n",
    "# rmse 계산을 위한 함수 생성\n",
    "def rmse(y_test, y_pred):\n",
    "    return np.sqrt(mean_squared_error(np.expm1(y_test), np.expm1(y_pred)))\n",
    "\n",
    "random_state=2020        # 하지만 우리는 이렇게 고정값을 세팅해 두겠습니다. \n",
    "\n",
    "gboost = GradientBoostingRegressor(random_state=random_state)\n",
    "xgboost = XGBRegressor(random_state=random_state)\n",
    "lightgbm = LGBMRegressor(random_state=random_state)\n",
    "rdforest = RandomForestRegressor(random_state=random_state)\n",
    "\n",
    "models = [gboost, xgboost, lightgbm, rdforest]\n",
    "\n",
    "def get_scores(models, train, y):\n",
    "    df = {}\n",
    "    \n",
    "    for model in models:\n",
    "        model_name = model.__class__.__name__\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(train, y, random_state=random_state, test_size=0.2)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        df[model_name] = rmse(y_test, y_pred)\n",
    "        score_df = pd.DataFrame(df, index=['RMSE']).T.sort_values('RMSE', ascending=False)\n",
    "            \n",
    "    return score_df\n",
    "\n",
    "get_scores(models, train, y)\n",
    "\n",
    "def my_GridSearch(model, train, y, param_grid, verbose=2, n_jobs=5):\n",
    "    # GridSearchCV 모델로 초기화\n",
    "    grid_model = GridSearchCV(model, param_grid=param_grid, scoring='neg_mean_squared_error', \\\n",
    "                              cv=5, verbose=verbose, n_jobs=n_jobs)\n",
    "    \n",
    "    # 모델 fitting\n",
    "    grid_model.fit(train, y)\n",
    "\n",
    "    # 결과값 저장\n",
    "    params = grid_model.cv_results_['params']\n",
    "    score = grid_model.cv_results_['mean_test_score']\n",
    "    \n",
    "    # 데이터 프레임 생성\n",
    "    results = pd.DataFrame(params)\n",
    "    results['score'] = score\n",
    "    \n",
    "    # RMSLE 값 계산 후 정렬\n",
    "    results['RMSLE'] = np.sqrt(-1 * results['score'])\n",
    "    results = results.sort_values('RMSLE')\n",
    "\n",
    "    return results\n",
    "\n",
    "# 그리드 탐색\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100],\n",
    "    'max_depth': [1, 10],\n",
    "}\n",
    "\n",
    "# 학습결과 예측\n",
    "\"\"\"\n",
    "model = LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "\n",
    "def save_submission(model, train, y, test, model_name, rmsle=None):\n",
    "    model.fit(train, y)\n",
    "    prediction = model.predict(test)\n",
    "    prediction = np.expm1(prediction)\n",
    "    data_dir = os.getenv('HOME')+'/aiffel/kaggle_kakr_housing/data'\n",
    "    submission_path = join(data_dir, 'sample_submission.csv')\n",
    "    submission = pd.read_csv(submission_path)\n",
    "    submission['price'] = prediction\n",
    "    submission_csv_path = '{}/submission_{}_RMSLE_{}.csv'.format(data_dir, model_name, rmsle)\n",
    "    submission.to_csv(submission_csv_path, index=False)\n",
    "    print('{} saved!'.format(submission_csv_path))\n",
    "\"\"\"\n",
    "\n",
    "#save_submission(model, train, y, test, 'lgbm', rmsle='0.164399')\n",
    "\n",
    "model_a = GradientBoostingRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "# GradientBoostingRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "model_b = XGBRegressor(max_depth=10, n_estimators=50, random_state=random_state)\n",
    "# XGBRegressor(max_depth=10, n_estimators=50, random_state=random_state)\n",
    "model_c = LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "# LGBMRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "model_d = RandomForestRegressor(max_depth=10, n_estimators=100, random_state=random_state) \n",
    "                                \n",
    "models = [{'model':model_a, 'name':'GradientBoosting'}, {'model':model_c, 'name':'XGBoost'},\n",
    "          {'model':model_b, 'name':'LightGBM'}, {'model':model_d, 'name':'RandomForest'}]\n",
    "                                \n",
    "# RandomForestRegressor(max_depth=10, n_estimators=100, random_state=random_state)\n",
    "\n",
    "#print(my_GridSearch(model_a, train, y, param_grid, verbose=2, n_jobs=5))\n",
    "#print(my_GridSearch(model_b, train, y, param_grid, verbose=2, n_jobs=5))\n",
    "#print(my_GridSearch(model_c, train, y, param_grid, verbose=2, n_jobs=5))\n",
    "#print(my_GridSearch(model_d, train, y, param_grid, verbose=2, n_jobs=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2983db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def AveragingBlending(models, x, y, sub_x):\n",
    "    for m in models : \n",
    "        m['model'].fit(x.values, y)\n",
    "    \n",
    "    predictions = np.column_stack([\n",
    "        m['model'].predict(sub_x.values) for m in models\n",
    "    ])\n",
    "    return np.mean(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5722fe44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f7d3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
